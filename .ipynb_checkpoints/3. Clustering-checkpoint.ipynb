{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling (1/2 - Clustering)\n",
    "\n",
    "Now that we have gone through the process of collating all the data into a single place, we can start to do some modelling!\n",
    "\n",
    "There are two broad aims here:\n",
    "\n",
    "**1. Clustering Analysis:** Which seats would an algorithm suggest are most alike, based on their characteristics? This might allow a party to see which seats really ought to be target seats, but aren't.\n",
    "\n",
    "\n",
    "**2. Categorisation Machine Learning:** Creating a model to predict what kind of seat each constituency is, based on its characteristics, and seeing which issues are the most influential in deciding this.\n",
    "\n",
    "# 1) Import libraries\n",
    "\n",
    "We have several libraries to import in order to carry out these analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "#For data visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.font_manager\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "#For data pre-processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#For clustering analysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#For evaluating clusters\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "#Suppress warnings from showing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Allow ourselves to save things\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define colours for the visuals\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "\n",
    "CB91_BlueD = '#016794'\n",
    "CB91_GreenD = '#187970'\n",
    "CB91_PinkD = '#B317B1'\n",
    "CB91_PurpleD = '#4E1762'\n",
    "CB91_VioletD = '#330E4C'\n",
    "CB91_AmberD = '#985E09'\n",
    "\n",
    "CB91_BlueL = '#ABE5FF'\n",
    "CB91_GreenL = '#B5F1EB'\n",
    "CB91_PinkL = '#FAD9FA'\n",
    "CB91_PurpleL = '#D9A8EB'\n",
    "CB91_VioletL = '#ECD4F5'\n",
    "CB91_AmberL = '#F9D094'\n",
    "\n",
    "\n",
    "#The following gradients will be used for heatmaps, etc\n",
    "CB91_Grad_BP = ['#2CBDFE', '#2fb9fc', '#33b4fa', '#36b0f8',\n",
    "                '#3aacf6', '#3da8f4', '#41a3f2', '#449ff0',\n",
    "                '#489bee', '#4b97ec', '#4f92ea', '#528ee8',\n",
    "                '#568ae6', '#5986e4', '#5c81e2', '#607de0',\n",
    "                '#6379de', '#6775dc', '#6a70da', '#6e6cd8',\n",
    "                '#7168d7', '#7564d5', '#785fd3', '#7c5bd1',\n",
    "                '#7f57cf', '#8353cd', '#864ecb', '#894ac9',\n",
    "                '#8d46c7', '#9042c5', '#943dc3', '#9739c1',\n",
    "                '#9b35bf', '#9e31bd', '#a22cbb', '#a528b9',\n",
    "                '#a924b7', '#ac20b5', '#b01bb3', '#b317b1']\n",
    "\n",
    "CB91_Grad_BA = ['#2cbdfe', '#31bdf9', '#36bcf5', '#3bbcf0',\n",
    "                '#41bcec', '#46bbe7', '#4bbbe3', '#50bbde',\n",
    "                '#55bbd9', '#5abad5', '#60bad0', '#65bacc',\n",
    "                '#6ab9c7', '#6fb9c3', '#74b9be', '#79b8ba',\n",
    "                '#7eb8b5', '#84b8b0', '#89b7ac', '#8eb7a7',\n",
    "                '#93b7a3', '#98b79e', '#9db69a', '#a3b695',\n",
    "                '#a8b690', '#adb58c', '#b2b587', '#b7b583',\n",
    "                '#bcb47e', '#c1b47a', '#c7b475', '#ccb371',\n",
    "                '#d1b36c', '#d6b367', '#dbb363', '#e0b25e',\n",
    "                '#e6b25a', '#ebb255', '#f0b151', '#f5b14c']\n",
    "\n",
    "CB91_Grad_AP = ['#f5b14c', '#f3ae4f', '#f0aa52', '#eea755',\n",
    "                '#eca458', '#eaa05c', '#e79d5f', '#e59962',\n",
    "                '#e39665', '#e19368', '#de8f6b', '#dc8c6e',\n",
    "                '#da8971', '#d88574', '#d58277', '#d37f7b',\n",
    "                '#d17b7e', '#cf7881', '#cc7584', '#ca7187',\n",
    "                '#c86e8a', '#c66a8d', '#c36790', '#c16493',\n",
    "                '#bf6096', '#bd5d9a', '#ba5a9d', '#b856a0',\n",
    "                '#b653a3', '#b450a6', '#b14ca9', '#af49ac',\n",
    "                '#ad46af', '#ab42b2', '#a83fb5', '#a63bb9',\n",
    "                '#a438bc', '#a235bf', '#9f31c2', '#9d2ec5']\n",
    "\n",
    "CB91_Grad_GP = ['#47dbcd', '#4bd9ce', '#50d8cf', '#54d6d0',\n",
    "                '#59d5d1', '#5dd3d2', '#61d2d3', '#66d0d4',\n",
    "                '#6acfd5', '#6fcdd6', '#73ccd6', '#78cad7',\n",
    "                '#7cc9d8', '#80c7d9', '#85c6da', '#89c4db',\n",
    "                '#8ec3dc', '#92c1dd', '#96c0de', '#9bbedf',\n",
    "                '#9fbde0', '#a4bbe1', '#a8bae2', '#acb8e3',\n",
    "                '#b1b7e4', '#b5b5e5', '#bab4e6', '#beb2e7',\n",
    "                '#c2b1e8', '#c7afe9', '#cbaee9', '#d0acea',\n",
    "                '#d4abeb', '#d9a9ec', '#dda8ed', '#e1a6ee',\n",
    "                '#e6a5ef', '#eaa3f0', '#efa2f1', '#f3a0f2']\n",
    "\n",
    "CB91_Grad_GWP= ['#47dbcd','#4fdcce','#56ddd0','#5dded1',\n",
    "                '#64dfd2','#6ae0d3','#70e1d5','#75e2d6',\n",
    "                '#7be3d7','#80e4d8','#85e5da','#8ae6db',\n",
    "                '#8fe7dc','#94e8dd','#98e9df','#9deae0',\n",
    "                '#a1ebe1','#a6ece2','#aaede4','#afede5',\n",
    "                '#b3eee6','#b7efe8','#bbf0e9','#c0f1ea',\n",
    "                '#c4f2eb','#c8f3ed','#ccf4ee','#d0f5ef',\n",
    "                '#d4f6f1','#d8f6f2','#dcf7f3','#e0f8f5',\n",
    "                '#e4f9f6','#e8faf7','#ecfbf8','#f0fcfa',\n",
    "                '#f3fcfb','#f7fdfc','#fbfefe','#ffffff',\n",
    "                '#fdfafe','#fbf5fc','#f9f0fb','#f7eaf9',\n",
    "                '#f4e5f8','#f2e0f7','#f0dbf5','#eed6f4',\n",
    "                '#ecd1f2','#e9ccf1','#e7c7f0','#e5c1ee',\n",
    "                '#e2bced','#e0b7eb','#deb2ea','#dbade8',\n",
    "                '#d9a8e7','#d6a3e5','#d49ee4','#d199e2',\n",
    "                '#cf94e1','#cc8fdf','#ca89de','#c784dc',\n",
    "                '#c57fdb','#c27ad9','#bf75d8','#bd6fd6',\n",
    "                '#ba6ad5','#b765d3','#b45fd2','#b25ad0',\n",
    "                '#af54cf','#ac4ecd','#a949cb','#a642ca',\n",
    "                '#a33cc8','#a035c7','#9d2ec5']\n",
    "\n",
    "CB91_Grad_BWP= ['#2cbdfe','#31bffe','#37c0fe','#3cc2fe',\n",
    "                '#42c4fe','#47c5fe','#4cc7fe','#52c9fe',\n",
    "                '#57cbfe','#5dccfe','#62cefe','#68d0fe',\n",
    "                '#6dd1fe','#72d3fe','#78d5fe','#7dd6fe',\n",
    "                '#83d8fe','#88dafe','#8ddbfe','#93ddfe',\n",
    "                '#98dfff','#9ee1ff','#a3e2ff','#a8e4ff',\n",
    "                '#aee6ff','#b3e7ff','#b9e9ff','#beebff',\n",
    "                '#c3ecff','#c9eeff','#cef0ff','#d4f1ff',\n",
    "                '#d9f3ff','#dff5ff','#e4f7ff','#e9f8ff',\n",
    "                '#effaff','#f4fcff','#fafdff','#ffffff',\n",
    "                '#fdfafe','#fbf5fc','#f9f0fb','#f7eaf9',\n",
    "                '#f4e5f8','#f2e0f7','#f0dbf5','#eed6f4',\n",
    "                '#ecd1f2','#e9ccf1','#e7c7f0','#e5c1ee',\n",
    "                '#e2bced','#e0b7eb','#deb2ea','#dbade8',\n",
    "                '#d9a8e7','#d6a3e5','#d49ee4','#d199e2',\n",
    "                '#cf94e1','#cc8fdf','#ca89de','#c784dc',\n",
    "                '#c57fdb','#c27ad9','#bf75d8','#bd6fd6',\n",
    "                '#ba6ad5','#b765d3','#b45fd2','#b25ad0',\n",
    "                '#af54cf','#ac4ecd','#a949cb','#a642ca',\n",
    "                '#a33cc8','#a035c7','#9d2ec5']\n",
    "\n",
    "#Add party colors\n",
    "con_blue = '#0A3B7C'\n",
    "lab_red = '#E4003B'\n",
    "lib_yel = '#FAA61A'\n",
    "snp_yel = '#FFF481'\n",
    "green_green = '#52DF00'\n",
    "brex_blue = '#00E2ED'\n",
    "ukip_pur = '#470A65'\n",
    "plaid_green = '#006A56'\n",
    "\n",
    "con_lab = '#992281'\n",
    "con_lib = '#837859'\n",
    "con_snp = '#85987f'\n",
    "lab_lib = '#ef532b'\n",
    "lab_snp = '#f27a5e'\n",
    "lib_snp = '#fccf4d'\n",
    "\n",
    "\n",
    "#A list that we'll use to cycle through colors in charts\n",
    "color_list = [CB91_Blue, CB91_Green, CB91_Amber, CB91_Pink,\n",
    "              CB91_Violet, CB91_BlueD, CB91_GreenD, CB91_Purple,\n",
    "              CB91_AmberL, CB91_BlueL, CB91_GreenL, CB91_AmberD, \n",
    "              CB91_VioletD, CB91_PinkL, CB91_VioletL, CB91_PinkD]\n",
    "\n",
    "\n",
    "#Use seaborn to set all the default chart visual settings\n",
    "sns.set(font='Franklin Gothic Book',\n",
    "        rc={\n",
    " 'axes.axisbelow': False,\n",
    " 'axes.edgecolor': 'lightgrey',\n",
    " 'axes.facecolor': 'white',\n",
    " 'axes.grid': False,\n",
    " 'axes.labelcolor': 'dimgrey',\n",
    " 'axes.spines.right': False,\n",
    " 'axes.spines.top': False,\n",
    " 'figure.facecolor': 'white',\n",
    " 'lines.solid_capstyle': 'round',\n",
    " 'patch.edgecolor': 'w',\n",
    " 'patch.force_edgecolor': True,\n",
    " 'text.color': 'dimgrey',\n",
    " 'xtick.bottom': False,\n",
    " 'xtick.color': 'dimgrey',\n",
    " 'xtick.direction': 'out',\n",
    " 'xtick.top': False,\n",
    " 'ytick.color': 'dimgrey',\n",
    " 'ytick.direction': 'out',\n",
    " 'ytick.left': False,\n",
    " 'ytick.right': False})\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"font.size\":16,\n",
    "                                \"axes.titlesize\":20,\n",
    "                                \"axes.labelsize\":16})\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the mapping function from the previous workbooks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e8e96a55fdf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#999999'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             grad=CB91_Grad_AP, data=df):\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     '''\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def gradient_mapper(kpi, grad, outliers=None, stretch=1, factor=1):\n",
    "    \n",
    "    '''\n",
    "    Takes a list/series of numbers, outputs a list of hex colours,\n",
    "    appropriate for heatmapping the initial data.\n",
    "    \n",
    "    Parameters:\n",
    "    - col (list/series, etc.): The data to be transformed\n",
    "    - grad (list hex codes): A list of colors that the data will be transformed to\n",
    "    - outliers (top,bottom,both): Stretches the outliers, resulting in more gradient\n",
    "                                  change amongst clustered values\n",
    "    - stretch (int): The number of colors to duplicate if outliers variable used\n",
    "    - factor (int): The scale of color duplication if outliers variable used\n",
    "    \n",
    "    '''\n",
    "    #Work out how many colours we have in the given gradient\n",
    "    colors = len(grad)\n",
    "    half = colors // 2\n",
    "    \n",
    "    #Ensure that stretch is possible\n",
    "    stretch = min(half//3, stretch)\n",
    "    \n",
    "    factors = [4*factor, 3*factor, 2*factor]\n",
    "    \n",
    "    if outliers != None:\n",
    "        #Stretch gradient if required. Declare three lists:\n",
    "        #Start is the stretch map for the bottom end\n",
    "        if (outliers.lower() == 'bottom') or (outliers.lower() == 'both'):\n",
    "            start = [factors[0]]*stretch + [factors[1]]*stretch + [factors[2]]*stretch\n",
    "        else:\n",
    "            start = []\n",
    "\n",
    "        #End is the stretch map for the top end\n",
    "        if (outliers.lower() == 'top') or (outliers.lower() == 'both'):\n",
    "            end = [factors[2]]*stretch + [factors[1]]*stretch + [factors[0]]*stretch\n",
    "        else:\n",
    "            end = []\n",
    "\n",
    "        #Middle is a list of 1s which will be non-transformed\n",
    "        middle = [1 for i in range(colors - len(start) - len(end))]\n",
    "\n",
    "        stretch_map = start + middle + end\n",
    "        \n",
    "    else:\n",
    "        stretch_map = [1 for i in range(colors)]\n",
    "        \n",
    "    #Create tuples of the gradients, and the number of\n",
    "    #times they should be repeated in the list\n",
    "    zip_list = list(zip(grad,stretch_map))\n",
    "    \n",
    "    #Use this to create a list of lists\n",
    "    #Each element will be a list of the same gradient\n",
    "    #repeated the required number of times\n",
    "    list_of_lists = [[i[0]]*i[1] for i in zip_list]\n",
    "    \n",
    "    #Melt this list of lists into a single list\n",
    "    grad = sum(list_of_lists, [])\n",
    "    \n",
    "    #Re-define colors variable\n",
    "    colors = len(grad)-1\n",
    "    \n",
    "    #Define the lowest and the highest points in the dataset\n",
    "    kpi_min = kpi.min()\n",
    "    kpi_max = kpi.max()\n",
    "    \n",
    "    #Transform the data to integers between zero and the length of the gradient list\n",
    "    first_map = list(map(lambda x: int(round(colors*(x-kpi_min) /\n",
    "                                             (kpi_max-kpi_min),0)), list(kpi)))\n",
    "    \n",
    "    #Map the integers onto the gradient list\n",
    "    second_map = list(map(lambda x: grad[x], first_map))\n",
    "    \n",
    "    #Return this, as well as the new gradient\n",
    "    return second_map, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f = open('Datasets/constituencies.hexjson')\n",
    "datamap = json.load(f)\n",
    "datamap = pd.DataFrame(datamap['hexes']).T\n",
    "datamap = datamap[['n','q','r']]\n",
    "datamap.columns=['Name','X','Y']\n",
    "\n",
    "def kpi_map(kpi, width=6, colorbar=True,\n",
    "            outliers=None, stretch=1, factor=1,\n",
    "            exclude=[], title=None,\n",
    "            colors=None, exc_color='#999999',\n",
    "            grad=CB91_Grad_AP, data=df):\n",
    "    \n",
    "    '''\n",
    "    Outputs a choropleth map, showing each constituency in the same size.\n",
    "    \n",
    "    Parameters:\n",
    "    - kpi (list/series, etc.): The data to be transformed\n",
    "    - width (float): The desired width of the figure\n",
    "    - df (dataframe): The dataframe to get the data from\n",
    "    - grad (list hex codes): A list of colors that the data will be transformed to\n",
    "    - outliers (top,bottom,both): Stretches the outliers, resulting in more gradient\n",
    "                                  change amongst clustered values\n",
    "    - stretch (int): The number of colors to duplicate if outliers variable used\n",
    "    - factor (int): The scale of color duplication if outliers variable used\n",
    "    - exclude (list): A list of regions to exclude from the chart\n",
    "    - colors (dataframe): A dataframe of hex-codes (index should be constituency codes)\n",
    "    - title (string): The desired title of the chart\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    #Filter out different regions, depending on paramaters\n",
    "    df_filtered = pd.concat([datamap, data[['Region',kpi]]], axis=1)\n",
    "    \n",
    "    #If we have colors to add, concatonate these in\n",
    "    if isinstance(colors, pd.DataFrame):\n",
    "        df_filtered = pd.concat([df_filtered, colors], axis=1)\n",
    "        df_filtered.columns = ['Name', 'X', 'Y', 'Region', kpi, 'Colors']\n",
    "    \n",
    "    df_filtered = df_filtered.loc[~df_filtered['Region'].isin(exclude)]\n",
    "    kpi_filtered = df_filtered[kpi]\n",
    "    \n",
    "    if isinstance(colors, pd.DataFrame) is False:\n",
    "        #Use the gradient mapper function to return the colors for the plot\n",
    "        gradient_map = gradient_mapper(kpi=kpi_filtered,\n",
    "                                    grad=grad,\n",
    "                                    outliers=outliers,\n",
    "                                    stretch=stretch,\n",
    "                                    factor=factor)\n",
    "        colors_map = gradient_map[0]\n",
    "    \n",
    "    else:\n",
    "        #Fill in nan colors with white\n",
    "        df_filtered['Colors'].fillna(exc_color, inplace=True)\n",
    "        \n",
    "        #Return the column to be used as the colours list in the plot\n",
    "        colors_map = list(df_filtered['Colors'])\n",
    "    \n",
    "    #Work out the aspect ratio of the filtered constituencies\n",
    "    X_diff = np.max(df_filtered['X'])-np.min(df_filtered['X'])\n",
    "    Y_diff = np.max(df_filtered['Y'])-np.min(df_filtered['Y'])\n",
    "    \n",
    "    #Declare the width and height of the plot\n",
    "    height = width * (Y_diff/X_diff)\n",
    "    size = 500*math.pi*((width/X_diff)**2)\n",
    "    \n",
    "    #Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(width,height))\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    #Plot the scatter\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "    ax1.scatter(df_filtered['X'],\n",
    "                df_filtered['Y'],\n",
    "                s=size,\n",
    "                marker='s',\n",
    "                c=colors_map)\n",
    "    \n",
    "    #Remove axes\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    ax1.set_title(title);\n",
    "    \n",
    "    #plot the colorbar\n",
    "    if (colorbar == True) and isinstance(colors, pd.DataFrame) is False:        \n",
    "        cmap = LinearSegmentedColormap.from_list(name= '',\n",
    "                                                 colors=gradient_map[1])\n",
    "        ax2 = fig.add_subplot(2,30,28)\n",
    "        norm = mpl.colors_map.Normalize(vmin=df_filtered[kpi].min(),\n",
    "                                    vmax=df_filtered[kpi].max())\n",
    "        cb = mpl.colorbar.ColorbarBase(ax2, cmap=cmap,\n",
    "                                       norm=norm, orientation='vertical')\n",
    "    \n",
    "        # remove the x and y ticks\n",
    "        for ax in [ax1,ax2]:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    else:\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Final Data Clean\n",
    "\n",
    "We have prepared a dataset in the previous workbooks. We will see that there are a few different iterations of the data that we could create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/data_with_targets.csv')\n",
    "df.rename(columns={'Unnamed: 0':'ID'}, inplace=True)\n",
    "df.set_index('ID', inplace=True, drop=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to clean up here.\n",
    "\n",
    "Firstly, we have some NAs. This is driven by regional data issues. Features with 18 NAs are where we had no data for Northern Ireland, and Features with 117 NAs is where we only had data for England.\n",
    "\n",
    "Given that the target values for Northern Ireland feature parties that are exclusive to Northern Ireland, data about these constituencies is actually not that helpful. We can probably afford to drop these in all cases.\n",
    "\n",
    "### The Speaker and Their Constituency\n",
    "\n",
    "As another complication, we should also consider the constituency held by the 'Speaker' of the House of Commons. The Speaker acts as the 'chairperson' of the House of Commons, and is chosen from the current crop of MPs *by* other MPs.\n",
    "\n",
    "<img src=\"images/bercow.gif\" alt=\"The Speaker\" style=\"width: 400px;\"/>\n",
    "\n",
    "<i><center>\"Division! Clear the lobby!\"</center></i>\n",
    "\n",
    "It is tradition that the seat held by the Speaker is not contested by the other main parties (the speaker is still elected as an MP, but stands as an 'independent'). Thus, results in these constituencies will be outliers in the dataset. We should probably exclude the seat of Chorley, seat of the speaker in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Finding the features with NAs\n",
    "df.loc[:,df.isna().sum() > 0].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Northern Ireland, Chorley\n",
    "df = df.loc[(df['Region'] != 'Northern Ireland')\n",
    "           &(df['Name'] != 'Chorley')]\n",
    "\n",
    "#We can also remove the constituency name column, which we don't really care about\n",
    "df_name = df.iloc[:,0]\n",
    "df_name.to_csv('Datasets/names.csv')\n",
    "\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to one-hot encode the 'region' and 'type' columns, becuase they are categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create the ohe object\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "#Fit it to the categorical columns\n",
    "ohe.fit(df[['Region','Type']])\n",
    "df_ohe = ohe.transform(df[['Region','Type']]).toarray()\n",
    "\n",
    "#Create the dataframe\n",
    "df_ohe = pd.DataFrame(df_ohe, index=df.index,\n",
    "                      columns=ohe.get_feature_names(['Region','Type']))\n",
    "\n",
    "#Join the dataframe back\n",
    "df = pd.concat([df_ohe,df.iloc[:,2:]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then spin off the datasets - one for 'Great Britain' (GB), which covers England, Wales, and Scotland, and a separate one for England alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Great Britain dataset will remove columns where we have NAs\n",
    "df_gb = df.dropna(axis=1)\n",
    "\n",
    "#England dataset will remove rows where we have NAs\n",
    "df_eng = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this has worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('* GB dataframe Scotland/Wales Const: ',\n",
    "      df_gb['Region_Scotland'].sum()+df_gb['Region_Wales'].sum())\n",
    "print('* GB dataframe NAs: ', df_gb.isna().sum().sum())\n",
    "\n",
    "\n",
    "print('\\n* England Scotland Const: ',\n",
    "      df_eng['Region_Scotland'].sum()+df_eng['Region_Wales'].sum())\n",
    "print('* England dataframe NAs: ', df_eng.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that Scotland and Wales are included in the GB dataframe, but not in the England dataframe, and we have no NAs in either dataframe. This is what we want.\n",
    "\n",
    "Let's save these datasets down for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb.to_csv('Datasets/data_gb.csv')\n",
    "df_eng.to_csv('Datasets/data_eng.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to a bit more cleaning before we can let scikit learn loose on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's separate the independent and dependent variables for both dataframes\n",
    "X_gb = df_gb.iloc[:,:-3]\n",
    "y17_gb = df_gb['Winner_17']\n",
    "y17_st_gb = df_gb['seat_types_17']\n",
    "y19_st_gb = df_gb['seat_types_yg']\n",
    "\n",
    "X_eng = df_eng.iloc[:,:-3]\n",
    "y17_eng = df_eng['Winner_17']\n",
    "y17_st_eng = df_eng['seat_types_17']\n",
    "y19_st_eng = df_eng['seat_types_yg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Cluster Analysis\n",
    "\n",
    "### K-Means Clustering\n",
    "Remember - with the clustering analysis we're simply trying to see which constituencies are most similar based on their characteristics. We would then want to compare these clusters, to see if they also tend to vote along similar lines.\n",
    "\n",
    "Given that the clustering algorithms are predicated on distance metrics, the first thing we need to do when we cluster is to perform feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_gb_scaled = scaler.fit_transform(X_gb)\n",
    "X_eng_scaled = scaler.fit_transform(X_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the clustering analysis with the overall Great Britain dataset and England datasets separately.\n",
    "\n",
    "We'll run the KMeans algorithm over a range of values of n.\n",
    "\n",
    "We use the Calinski Harabasz score as a metric, since we're not actually interested in the algorithm *correctly* assigning constituencies to the 'right' cluster - rather, we are looking for anomalous labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare k_values\n",
    "k_values = range(3,50)\n",
    "\n",
    "#Initiate some empty lists for the GB dataset\n",
    "km_preds_gb = []\n",
    "km_cs_scores_gb = []\n",
    "\n",
    "#Iterate through these\n",
    "for k in k_values:\n",
    "    #Instantiate and run a KMeans algorithm\n",
    "    k_means = KMeans(n_clusters=k)\n",
    "    k_means.fit(X_gb_scaled)\n",
    "    \n",
    "    #Store the predicted labels\n",
    "    km_preds_gb.append(k_means.predict(X_gb_scaled))\n",
    "    \n",
    "    #Evaluate and store the clusters' Calinski Harabasz score\n",
    "    cs_score = calinski_harabasz_score(X_gb_scaled, k_means.labels_)\n",
    "    km_cs_scores_gb.append(cs_score)\n",
    "    \n",
    "\n",
    "#Now initiate some empty lists for the English dataset\n",
    "km_preds_eng = []\n",
    "km_cs_scores_eng = []\n",
    "\n",
    "#Iterate through these values of k\n",
    "for k in k_values:\n",
    "    #Instantiate and run a KMeans algorithm\n",
    "    k_means = KMeans(n_clusters=k)\n",
    "    k_means.fit(X_eng_scaled)\n",
    "    \n",
    "    #Store the predicted labels\n",
    "    km_preds_eng.append(k_means.predict(X_eng_scaled))\n",
    "    \n",
    "    #Evaluate and store the clusters' Calinski Harabasz score\n",
    "    cs_score = calinski_harabasz_score(X_eng_scaled, k_means.labels_)\n",
    "    km_cs_scores_eng.append(cs_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Agglomerative Clustering\n",
    "\n",
    "Let's now create clusters using HAC.\n",
    "\n",
    "Once we do this, we'll be able to compare the different clusters created by each algorithm, and see which ones came out better. Let's use the same approach that we used for the k-means algorithm above - deploying the algorithms for both the GB dataset, and the England dataset separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate some empty lists for the GB dataset\n",
    "hac_preds_gb = []\n",
    "hac_cs_scores_gb = []\n",
    "\n",
    "#Iterate through the k values we'd previously defined\n",
    "for k in k_values:\n",
    "    #Instantiate and run a HAC algorithm\n",
    "    agg_clust = AgglomerativeClustering(n_clusters=k)\n",
    "    preds = agg_clust.fit_predict(X_gb_scaled)\n",
    "    \n",
    "    #Store the predicted labels\n",
    "    hac_preds_gb.append(preds)\n",
    "    \n",
    "    #Evaluate and store the clusters' Calinski Harabasz score\n",
    "    cs_score = calinski_harabasz_score(X_gb_scaled, preds)\n",
    "    hac_cs_scores_gb.append(cs_score)\n",
    "    \n",
    "\n",
    "#Now initiate some empty lists for the English dataset\n",
    "hac_preds_eng = []\n",
    "hac_cs_scores_eng = []\n",
    "\n",
    "for k in k_values:\n",
    "    #Instantiate and run a HAC algorithm\n",
    "    agg_clust = AgglomerativeClustering(n_clusters=k)\n",
    "    preds = agg_clust.fit_predict(X_eng_scaled)\n",
    "    \n",
    "    #Store the predicted labels\n",
    "    hac_preds_eng.append(preds)\n",
    "    \n",
    "    #Evaluate and store the clusters' Calinski Harabasz score\n",
    "    cs_score = calinski_harabasz_score(X_eng_scaled, preds)\n",
    "    hac_cs_scores_eng.append(cs_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "plt.plot(k_values, km_cs_scores_gb, alpha=alpha,\n",
    "            label='GB Dataset - K-Means Clustering')\n",
    "\n",
    "plt.plot(k_values, km_cs_scores_eng, alpha=alpha,\n",
    "            label='England Dataset - K-Means Clustering')\n",
    "\n",
    "plt.plot(k_values, hac_cs_scores_gb, alpha=alpha,\n",
    "            label='GB Dataset - HAC')\n",
    "\n",
    "plt.plot(k_values, hac_cs_scores_eng, alpha=alpha,\n",
    "            label='England Dataset - HAC')\n",
    "\n",
    "plt.vlines(x=12, ymin=0, ymax=150, color='red')\n",
    "plt.ylim(20,150)\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "plt.title('Calinski Harabasz Scores for K-Means Clustering')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Calinski Harabasz Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in both cases, it seems like we hit the 'elbow' point at around 10-12 clusters, depending on the dataset. It does seem that the algorithm has an easier time clustering the English dataset. We would expect this - it has more features to work with.\n",
    "\n",
    "Moreover, HAC seems to produce better clusters at lower values of K (though the K-Means and HAC CS scores converge for their respective datasets from about k=30 onwards).\n",
    "\n",
    "We can also visualise our HAC clustering with dendrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ward() function\n",
    "linkage_array = ward(X_gb_scaled)\n",
    "\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances between clusters\n",
    "plt.figure(figsize=(12,5))\n",
    "dendrogram(linkage_array, truncate_mode='lastp', p=12)\n",
    "\n",
    "plt.title('Dendrogram of HAC Clustering on GB Dataset')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ward() function\n",
    "linkage_array = ward(X_eng_scaled)\n",
    "\n",
    "# Now we plot the dendrogram for the linkage_array containing the distances between clusters\n",
    "plt.figure(figsize=(12,5))\n",
    "dendrogram(linkage_array, truncate_mode='lastp', p=9)\n",
    "\n",
    "plt.title('Dendrogram of HAC Clustering on England Dataset')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Our Clusters\n",
    "\n",
    "Now we have our clusters, let's investigate them further by seeing how the different seat types break down amongst them. We will use the clusters from the HAC algorithm, seeing as this algorithm seemed to perform better.\n",
    "\n",
    "First, let's get our clusters at a GB level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cluster_yg = pd.concat([y19_st_gb,\n",
    "                           pd.DataFrame(hac_preds_gb[7],\n",
    "                                        index=y17_st_gb.index,\n",
    "                                        columns=['Cluster'])],\n",
    "                           axis=1)\n",
    "\n",
    "gb_cluster_yg_pivot = pd.pivot_table(data = gb_cluster_yg,\n",
    "                                     index='seat_types_yg',\n",
    "                                     columns='Cluster',\n",
    "                                     aggfunc=len,\n",
    "                                     fill_value=0)\n",
    "\n",
    "gb_cluster_yg_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how these clusters are arranged on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cluster_yg['Colors'] = gb_cluster_yg['Cluster'].map(\n",
    "    lambda x: color_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_map('Population',\n",
    "        colors=gb_cluster_yg[['Colors']],\n",
    "        data=pd.read_csv('Datasets/data_with_targets.csv').set_index('Unnamed: 0'),\n",
    "        exc_color='#ffffff',\n",
    "        exclude=['Northern Ireland'],\n",
    "        width=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now at the England clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng_cluster_yg = pd.concat([y19_st_eng,\n",
    "                           pd.DataFrame(hac_preds_eng[6],\n",
    "                                        index=y17_st_eng.index,\n",
    "                                        columns=['Cluster'])],\n",
    "                           axis=1)\n",
    "\n",
    "eng_cluster_yg_pivot = pd.pivot_table(data = eng_cluster_yg,\n",
    "                                     index='seat_types_yg',\n",
    "                                     columns='Cluster',\n",
    "                                     aggfunc=len,\n",
    "                                     fill_value=0)\n",
    "\n",
    "eng_cluster_yg_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_list = ['con safe','con lab marginal',\n",
    "              'lab safe','ld safe',\n",
    "              'con ld marginal','lab ld marginal',\n",
    "              'green safe']\n",
    "\n",
    "party_colors = [con_blue, con_lab, lab_red, lib_yel,\n",
    "               con_lib, lab_lib, green_green]\n",
    "\n",
    "eng_cluster_yg_pivot=eng_cluster_yg_pivot.reindex(party_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of zeros in these matrices suggest that the clustering has been somewhat successful. However, the political value in this exercise is spotting the low numbers.\n",
    "\n",
    "For example, if we look at cluster 3, we could say that these have the make up of Conservative safe seats. So there's no reason why the Conservatives shouldn't push in the 30 constituencies that are not yet 'safe'.\n",
    "\n",
    "Let's see this on a bar chart for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_cluster_bar = (eng_cluster_yg_pivot / eng_cluster_yg_pivot.sum()).T\n",
    "\n",
    "eng_cluster_bar.plot.barh(stacked=True,\n",
    "                          figsize=(12,5),\n",
    "                          width=0.8,\n",
    "                          color=party_colors)\n",
    "\n",
    "plt.xticks(ticks = np.arange(0,1.2,0.2), labels = np.arange(0,120,20))\n",
    "plt.xlim(0,1);\n",
    "\n",
    "plt.title('English Constituency Clusters');\n",
    "plt.ylabel('Cluster Number')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.85, -0.1),\n",
    "           ncol=4,\n",
    "           frameon=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how these clusters are arranged on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column that assigns a color to each cluster\n",
    "eng_cluster_yg['Colors'] = eng_cluster_yg['Cluster'].map(\n",
    "    lambda x: color_list[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_map('Population',\n",
    "        colors=eng_cluster_yg[['Colors']],\n",
    "        data=pd.read_csv('Datasets/data_with_targets.csv').set_index('Unnamed: 0'),\n",
    "        exc_color='#ffffff',\n",
    "        exclude=['Northern Ireland','Scotland','Wales'],\n",
    "        title='English Constituency Clusters',\n",
    "        width=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify the seats in cluster 3, which are not Conservative safe seats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_name[list(eng_cluster_yg.loc[(eng_cluster_yg['Cluster']==3)\n",
    "                  &(eng_cluster_yg['seat_types_yg']!='con safe')].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the sole cluster 3 constituency that is classified as a safe labour seat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name[list(eng_cluster_yg.loc[(eng_cluster_yg['Cluster']==3)\n",
    "                  &(eng_cluster_yg['seat_types_yg']=='lab safe')].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know which seats the Conservatives might want to chase. But what messages should they be pushing in these areas to push them over the line in these places?\n",
    "\n",
    "To help answer this last question, let's create a function that will print out a dashboard of KPIs for each cluster that we've created. Thus we will be able to see what the key unifying characteristics of each cluster are, which allow local messaging to be targeted more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cluster_IDs(cluster, cluster_data, cluster_col='Cluster'):\n",
    "    \n",
    "    '''\n",
    "    Takes a cluster label (1, 2, 3, etc.) and a cluster dataframe,\n",
    "    and returns a list of constituency IDs that belong to the given cluster\n",
    "    '''\n",
    "    return list(cluster_data.loc[cluster_data['Cluster']==cluster].index)\n",
    "\n",
    "\n",
    "\n",
    "def cluster_kpi(kpi, data, cluster_data, cluster_col='Cluster'):\n",
    "    \n",
    "    '''\n",
    "    For a given set of cluster labels, and a given KPI, returns a\n",
    "    dataframe with one row, for that given kpi\n",
    "    '''\n",
    "    \n",
    "    #Find the unique cluster labels\n",
    "    cluster_labels = sorted(list(cluster_data[cluster_col].unique()))\n",
    "    \n",
    "    #Declare an empty list to store the KPI data in\n",
    "    kpi_values = []\n",
    "    \n",
    "    #Iterate through the labels\n",
    "    for i in cluster_labels:\n",
    "        #Work out which constituencies are in the cluster\n",
    "        cluster_index = cluster_IDs(cluster = i,\n",
    "                                    cluster_data = cluster_data,\n",
    "                                    cluster_col = cluster_col)\n",
    "        \n",
    "        #Go to the data table, and find the mean for that kpi\n",
    "        #for those constituencies. Append to the list\n",
    "        mean = data.loc[cluster_index, kpi].mean()\n",
    "        kpi_values.append(mean)\n",
    "        \n",
    "    #Create and return a dataframe as required\n",
    "    return pd.DataFrame([kpi_values],\n",
    "                        columns = cluster_labels,\n",
    "                        index = [kpi])\n",
    "\n",
    "\n",
    "\n",
    "def cluster_kpis(kpis, data, cluster_data, cluster_col='Cluster'):\n",
    "    \n",
    "    '''\n",
    "    For a list of KPIs, return a dataframe showing mean\n",
    "    values on a cluster by cluster basis\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Declare an empty dataframe\n",
    "    df_temp = pd.DataFrame()\n",
    "    \n",
    "    #For each kpi, find the average cluster means, and append to the dataframe\n",
    "    for i in kpis:\n",
    "        cluster_values = cluster_kpi(kpi=i,\n",
    "                                     data=data,\n",
    "                                     cluster_data=cluster_data,\n",
    "                                     cluster_col=cluster_col)\n",
    "        \n",
    "        df_temp = pd.concat([df_temp, cluster_values])\n",
    "        \n",
    "    return df_temp\n",
    "        \n",
    "        \n",
    "\n",
    "def heatmap_cluster_kpis(kpis, data,\n",
    "                       cluster_data,\n",
    "                       size=0.4,\n",
    "                       cmap=CB91_Grad_BP,\n",
    "                       cluster_col='Cluster'):\n",
    "    \n",
    "    '''\n",
    "    Heatmap the table produced by the cluster_kpis function\n",
    "    '''\n",
    "    \n",
    "    #Calculate the required table and transpose\n",
    "    df_temp = cluster_kpis(kpis=kpis,\n",
    "                           data=data,\n",
    "                           cluster_data=cluster_data,\n",
    "                           cluster_col=cluster_col).T\n",
    "    \n",
    "    #For each kpi, scale as required\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df_temp)\n",
    "    df_scale = scaler.transform(df_temp)\n",
    "    \n",
    "    df_scale = pd.DataFrame(df_scale,\n",
    "                            columns=df_temp.columns,\n",
    "                            index=df_temp.index)\n",
    "    \n",
    "    height = size*len(df_scale.index)\n",
    "    width = 1.1*size*len(df_scale.columns)\n",
    "    \n",
    "    plt.figure(figsize=(width,height))\n",
    "    sns.heatmap(df_scale,cbar=True,cmap=cmap,\n",
    "               yticklabels=['A','B','C','D','E','F','G','H','I'])\n",
    "    plt.yticks(rotation='horizontal')\n",
    "    \n",
    "\n",
    "    \n",
    "#Define a standard list of KPIs to look at\n",
    "kpis_gb=['PopDensity', 'Type_Large City', 'Type_Large Town',\n",
    "      'Type_Rural', 'Type_Small City', 'Type_Small Town',\n",
    "      '2019Wage', 'HousePricePerWage', '%HousePriceGrowth',\n",
    "      '%OwnOutright','%OwnWithMort', '%PrivateRent',\n",
    "      '%SocialHousing','%Unemployment', 'UnemploymentChange',\n",
    "      '%Heavy Industry & Manufacturing', '%Wholesale & Retail',\n",
    "      '%FS & ICT', '%White', '%Muslim', '%BornUK', '%BornOtherEU',\n",
    "      '%Level4+', '%LeaveVote']\n",
    "\n",
    "kpis_eng=['PopDensity', 'Type_Large City', 'Type_Large Town',\n",
    "          'Type_Rural', 'Type_Small City', 'Type_Small Town',\n",
    "          '2019Wage', 'HousePricePerWage', '%HousePriceGrowth',\n",
    "          '%OwnOutright','%OwnWithMort', '%PrivateRent',\n",
    "          '%SocialHousing','%Unemployment', 'UnemploymentChange',\n",
    "          '%Heavy Industry & Manufacturing', '%Wholesale & Retail',\n",
    "          '%FS & ICT', '%White', '%BornUK', '%BornOtherEU', '%Muslim',\n",
    "          '%Level4+', '%ChildcareGood', 'LASpendGrowth15to19',\n",
    "          'DiseasesPerPop', 'Depression','%17Turnout','%LeaveVote']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a heatmap of the different clusters, which ranks each cluster along a range of KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatmap_cluster_kpis(kpis=kpis_eng,\n",
    "        data=df_eng,cmap=CB91_Grad_BWP[::-1],size=0.5,\n",
    "        cluster_data=eng_cluster_yg)\n",
    "\n",
    "plt.title('Heatmap of English Constituency Clusters by KPI\\n\\\n",
    "(0.0=Lowest Cluster For That KPI, 1.0=Highest Cluster For That KPI)\\n')\n",
    "plt.ylabel('Cluster Number');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can we say about Cluster 3?\n",
    "\n",
    "* The inhabitants live in small towns, and have relatively high rates of home ownership, with low levels of social housing. However, house prices haven't grown strongly.\n",
    "* A high share of people work in Heavy Industry, and wages are below average.\n",
    "* Most people are white and UK-born.\n",
    "* Early-years childcare is decent, but education levels are generally low.\n",
    "* Trend in local authority funding is poor (likely as the result of government cuts).\n",
    "* There is a high incidence of chronic illness.\n",
    "* Many people voted to leave the EU, and turnout before was relatively high.\n",
    "\n",
    "So a Conservative candidate campaigning in these specific seats can use this information to tailor their campaign messaging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
